{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import LSTM, Input, TimeDistributed, Dense, Activation, RepeatVector, Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.layers import Dropout\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import functools\n",
    "\n",
    "\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from keras.preprocessing.image import load_img\n",
    "from Encoder import Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image1</th>\n",
       "      <th>Image2</th>\n",
       "      <th>Clean_capt</th>\n",
       "      <th>Number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CXR162_IM-0401-1001</td>\n",
       "      <td>CXR162_IM-0401-2001</td>\n",
       "      <td>normal chest</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CXR1390_IM-0249-1001</td>\n",
       "      <td>CXR1390_IM-0249-2001</td>\n",
       "      <td>no evidence active disease</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CXR604_IM-2193-1001</td>\n",
       "      <td>CXR604_IM-2193-2001</td>\n",
       "      <td>no evidence active disease</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CXR2699_IM-1167-1001</td>\n",
       "      <td>CXR2699_IM-1167-2001</td>\n",
       "      <td>no acute cardiopulmonary disease</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CXR2841_IM-1253-2001</td>\n",
       "      <td>CXR2841_IM-1253-2001</td>\n",
       "      <td>no acute cardiopulmonary disease</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Image1                Image2  \\\n",
       "0   CXR162_IM-0401-1001   CXR162_IM-0401-2001   \n",
       "1  CXR1390_IM-0249-1001  CXR1390_IM-0249-2001   \n",
       "2   CXR604_IM-2193-1001   CXR604_IM-2193-2001   \n",
       "3  CXR2699_IM-1167-1001  CXR2699_IM-1167-2001   \n",
       "4  CXR2841_IM-1253-2001  CXR2841_IM-1253-2001   \n",
       "\n",
       "                         Clean_capt  Number  \n",
       "0                      normal chest       1  \n",
       "1        no evidence active disease       2  \n",
       "2        no evidence active disease       3  \n",
       "3  no acute cardiopulmonary disease       4  \n",
       "4  no acute cardiopulmonary disease       5  "
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('./train.csv')  \n",
    "test = pd.read_csv('./test.csv') \n",
    "data = pd.read_csv('./input.csv')\n",
    "dataset = data[[\"Image1\",\"Image2\",\"Clean_capt\",\"Number\"]]\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image1</th>\n",
       "      <th>Image2</th>\n",
       "      <th>Clean_capt</th>\n",
       "      <th>Number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CXR162_IM-0401-1001</td>\n",
       "      <td>CXR162_IM-0401-2001</td>\n",
       "      <td>normal chest</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CXR1390_IM-0249-1001</td>\n",
       "      <td>CXR1390_IM-0249-2001</td>\n",
       "      <td>no evidence active disease</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CXR604_IM-2193-1001</td>\n",
       "      <td>CXR604_IM-2193-2001</td>\n",
       "      <td>no evidence active disease</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Image1                Image2                  Clean_capt  \\\n",
       "0   CXR162_IM-0401-1001   CXR162_IM-0401-2001                normal chest   \n",
       "1  CXR1390_IM-0249-1001  CXR1390_IM-0249-2001  no evidence active disease   \n",
       "2   CXR604_IM-2193-1001   CXR604_IM-2193-2001  no evidence active disease   \n",
       "\n",
       "   Number  \n",
       "0       1  \n",
       "1       2  \n",
       "2       3  "
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.iloc[0:3,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_features(df, input_size):\n",
    "    path = '/Users/shuyuexu/Documents/COLUMBIA/2021fall/4995AML/project/NLMCXR_png'\n",
    "    \n",
    "    Xnet_features = {}\n",
    "    for i in range(len(df)):\n",
    "        f = os.path.join(path, df.iloc[i][\"Image1\"].lstrip())\n",
    "        f = f+\".png\"\n",
    "        image1 = Image.open(f)\n",
    "        image1 = image1.resize((input_size[0], input_size[1]),Image.NEAREST)\n",
    "        image1 = np.array(image1).reshape(1,input_size[0],input_size[1],input_size[2])/255\n",
    "        \n",
    "        f = os.path.join(path, df.iloc[i][\"Image2\"].lstrip())\n",
    "        f = f+\".png\"\n",
    "        \n",
    "        image2 = Image.open(f)\n",
    "        image2 = image2.resize((input_size[0], input_size[1]),Image.NEAREST)\n",
    "        image2 = np.array(image2).reshape(1,input_size[0],input_size[1],input_size[2])/255\n",
    "\n",
    "        en_model = Encoder()\n",
    "        en_model.build(input_shape = (None, input_size[0],input_size[1],input_size[2]))\n",
    "\n",
    "        image1_features = en_model.predict(image1)\n",
    "        image2_features = en_model.predict(image2)\n",
    "        input_concat = np.concatenate((image1_features, image2_features), axis=1)\n",
    "        \n",
    "        Xnet_features[int(df.iloc[i]['Number'])] = input_concat\n",
    "    return Xnet_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "en_output = get_img_features(test.iloc[0:20,], (224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2145: array([[4.9550453e-04, 2.0991680e-03, 1.7735970e-03, ..., 8.4199679e-01,\n",
       "         8.4065425e-01, 6.8691975e-01]], dtype=float32),\n",
       " 1051: array([[0.        , 0.00184126, 0.00116009, ..., 0.81290716, 0.81862867,\n",
       "         0.66535187]], dtype=float32),\n",
       " 1131: array([[4.6907488e-04, 2.1022649e-03, 2.4972956e-03, ..., 8.2906669e-01,\n",
       "         8.4035921e-01, 6.7798662e-01]], dtype=float32),\n",
       " 3616: array([[1.4932631e-04, 1.7147649e-03, 2.8314192e-03, ..., 8.4204483e-01,\n",
       "         8.4104896e-01, 6.8688482e-01]], dtype=float32),\n",
       " 3339: array([[7.0486767e-06, 2.1787314e-03, 1.8758981e-03, ..., 8.3596873e-01,\n",
       "         8.4641933e-01, 6.8505216e-01]], dtype=float32),\n",
       " 369: array([[3.8068678e-04, 1.8055823e-03, 1.5445231e-03, ..., 8.1980544e-01,\n",
       "         8.2468694e-01, 6.8049926e-01]], dtype=float32),\n",
       " 1955: array([[3.8213431e-04, 2.0339522e-03, 1.6955610e-03, ..., 8.3482862e-01,\n",
       "         8.2959968e-01, 6.8235379e-01]], dtype=float32),\n",
       " 3813: array([[5.5457797e-04, 2.3255087e-03, 1.6916335e-03, ..., 8.0783033e-01,\n",
       "         8.2563221e-01, 6.6535902e-01]], dtype=float32),\n",
       " 2113: array([[6.2002109e-05, 1.8069228e-03, 2.3020096e-03, ..., 8.2237083e-01,\n",
       "         8.3359653e-01, 6.7047399e-01]], dtype=float32),\n",
       " 1473: array([[4.0865655e-04, 2.3808961e-03, 1.3108701e-03, ..., 8.1712085e-01,\n",
       "         8.6061502e-01, 6.7571342e-01]], dtype=float32),\n",
       " 927: array([[6.0504820e-04, 2.1001147e-03, 1.9804998e-03, ..., 9.0649134e-01,\n",
       "         8.6976004e-01, 7.3983485e-01]], dtype=float32),\n",
       " 866: array([[2.6347482e-04, 1.9871220e-03, 3.4864335e-03, ..., 8.4328252e-01,\n",
       "         8.4209108e-01, 6.8990308e-01]], dtype=float32),\n",
       " 1968: array([[0.        , 0.00129507, 0.00227451, ..., 0.8949681 , 0.96462256,\n",
       "         0.74120605]], dtype=float32),\n",
       " 1324: array([[2.0767657e-04, 2.0857274e-03, 1.1943693e-03, ..., 7.4444431e-01,\n",
       "         8.4086668e-01, 6.1591518e-01]], dtype=float32),\n",
       " 1782: array([[9.6806280e-06, 1.9226091e-03, 1.9795087e-03, ..., 7.9815477e-01,\n",
       "         8.0505651e-01, 6.5576327e-01]], dtype=float32),\n",
       " 2766: array([[0.        , 0.00182092, 0.00222601, ..., 0.830097  , 0.8656252 ,\n",
       "         0.6747154 ]], dtype=float32),\n",
       " 1569: array([[3.5670062e-04, 2.2255220e-03, 1.7428959e-03, ..., 8.2828504e-01,\n",
       "         8.2490283e-01, 6.7652863e-01]], dtype=float32),\n",
       " 2372: array([[1.0892057e-04, 2.3758772e-03, 2.5955779e-03, ..., 8.0548805e-01,\n",
       "         8.4385246e-01, 6.6880488e-01]], dtype=float32),\n",
       " 36: array([[3.8029352e-04, 1.8439557e-03, 2.7074497e-03, ..., 8.1866837e-01,\n",
       "         8.5745722e-01, 6.7026895e-01]], dtype=float32),\n",
       " 2033: array([[4.3449336e-06, 2.1428221e-03, 1.7226979e-03, ..., 8.3789164e-01,\n",
       "         8.2778198e-01, 6.8506002e-01]], dtype=float32)}"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.9550453e-04, 2.0991680e-03, 1.7735970e-03, ..., 8.4199679e-01,\n",
       "        8.4065425e-01, 6.8691975e-01]], dtype=float32)"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_output[2145]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_size = 155\n",
    "BATCH_SIZE = 14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_img = train[\"Number\"]\n",
    "# X_cv_img = test[\"Number\"] \n",
    "# y_train_rep = train['Clean_capt'] \n",
    "# y_cv_rep = test['Clean_capt']\n",
    "\n",
    "# def load_image(id_, report):\n",
    "#     '''Loads the Image Features with their corresponding Ids'''\n",
    "#     img_feature = en_output[id_][0]\n",
    "#     return img_feature, report\n",
    "\n",
    "# def create_dataset(img_name_train, report_train):\n",
    "#     dataset = tf.data.Dataset.from_tensor_slices((img_name_train, report_train))\n",
    "#   # Use map to load the numpy files in parallel\n",
    "#     dataset = dataset.map(lambda item1, item2: tf.numpy_function(load_image, [item1, item2],\n",
    "#                           [tf.float32, tf.string]),\n",
    "#                           num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "#   # Shuffle and batch\n",
    "#     dataset = dataset.shuffle(500).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "#     return dataset\n",
    "\n",
    "# train_dataset = create_dataset(X_train_img, y_train_rep)\n",
    "# cv_dataset = create_dataset(X_cv_img, y_cv_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = test.iloc[0:20,]  ### using all test data instead\n",
    "X_train_img = c['Number']  \n",
    "y_train_rep = c[\"Clean_capt\"]\n",
    "\n",
    "## will need \n",
    "# X_test_img = test['Number']  \n",
    "# y_test_rep = train[\"Clean_capt\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(y_train_rep.values) \n",
    "vocab_size = len(tokenizer.word_index.keys()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         no acute disease\n",
       "1         no acute cardiopulmonary disease\n",
       "2                    negative chest x xxxx\n",
       "3                        no active disease\n",
       "4         no acute cardiopulmonary disease\n",
       "5                     no acute abnormality\n",
       "6     no acute cardiopulmonary abnormality\n",
       "7     no acute cardiopulmonary abnormality\n",
       "8     no acute cardiopulmonary abnormality\n",
       "9            no acute preoperative finding\n",
       "10              no acute pulmonary disease\n",
       "11    no acute cardiopulmonary abnormality\n",
       "12    no acute cardiopulmonary abnormality\n",
       "13        no acute cardiopulmonary finding\n",
       "14              no acute pulmonary disease\n",
       "15                       no active disease\n",
       "16    no acute cardiopulmonary abnormality\n",
       "17            heart size normal lung clear\n",
       "18    no acute cardiopulmonary abnormality\n",
       "19        no acute cardiopulmonary finding\n",
       "Name: Clean_capt, dtype: object"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[\"Clean_capt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     2145\n",
       "1     1051\n",
       "2     1131\n",
       "3     3616\n",
       "4     3339\n",
       "5      369\n",
       "6     1955\n",
       "7     3813\n",
       "8     2113\n",
       "9     1473\n",
       "10     927\n",
       "11     866\n",
       "12    1968\n",
       "13    1324\n",
       "14    1782\n",
       "15    2766\n",
       "16    1569\n",
       "17    2372\n",
       "18      36\n",
       "19    2033\n",
       "Name: Number, dtype: int64"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(id_, report):\n",
    "    '''Loads the Image Features with their corresponding Ids'''\n",
    "    img_feature = en_output[id_][0]\n",
    "    return img_feature, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_img, y_train_rep))\n",
    "#cv_dataset = tf.data.Dataset.from_tensor_slices((X_test_img, y_test_rep))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(lambda item1, item2: tf.numpy_function(load_image, [item1, item2],\n",
    "                          [tf.float32, tf.string]),\n",
    "                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "# cv_dataset = cv_dataset.map(lambda item1, item2: tf.numpy_function(load_image, [item1, item2],\n",
    "#                           [tf.float32, tf.string]),\n",
    "#                           num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(500).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "##cv_dataset = cv_dataset.shuffle(500).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2.0767657e-04 2.0857274e-03 1.1943693e-03 ... 7.4444431e-01\n",
      "  8.4086668e-01 6.1591518e-01]\n",
      " [6.2002109e-05 1.8069228e-03 2.3020096e-03 ... 8.2237083e-01\n",
      "  8.3359653e-01 6.7047399e-01]\n",
      " [0.0000000e+00 1.8412616e-03 1.1600930e-03 ... 8.1290716e-01\n",
      "  8.1862867e-01 6.6535187e-01]\n",
      " ...\n",
      " [4.3449336e-06 2.1428221e-03 1.7226979e-03 ... 8.3789164e-01\n",
      "  8.2778198e-01 6.8506002e-01]\n",
      " [0.0000000e+00 1.8209199e-03 2.2260072e-03 ... 8.3009702e-01\n",
      "  8.6562520e-01 6.7471540e-01]\n",
      " [3.5670062e-04 2.2255220e-03 1.7428959e-03 ... 8.2828504e-01\n",
      "  8.2490283e-01 6.7652863e-01]], shape=(14, 2048), dtype=float32)\n",
      "tf.Tensor(\n",
      "[b'no acute cardiopulmonary finding'\n",
      " b'no acute cardiopulmonary abnormality'\n",
      " b'no acute cardiopulmonary disease' b'no acute preoperative finding'\n",
      " b'no active disease' b'no acute cardiopulmonary abnormality'\n",
      " b'no acute cardiopulmonary disease' b'no acute pulmonary disease'\n",
      " b'no acute cardiopulmonary abnormality' b'no acute abnormality'\n",
      " b'heart size normal lung clear' b'no acute cardiopulmonary finding'\n",
      " b'no active disease' b'no acute cardiopulmonary abnormality'], shape=(14,), dtype=string)\n",
      "tf.Tensor(\n",
      "[[4.9550453e-04 2.0991680e-03 1.7735970e-03 ... 8.4199679e-01\n",
      "  8.4065425e-01 6.8691975e-01]\n",
      " [3.8213431e-04 2.0339522e-03 1.6955610e-03 ... 8.3482862e-01\n",
      "  8.2959968e-01 6.8235379e-01]\n",
      " [4.6907488e-04 2.1022649e-03 2.4972956e-03 ... 8.2906669e-01\n",
      "  8.4035921e-01 6.7798662e-01]\n",
      " [2.6347482e-04 1.9871220e-03 3.4864335e-03 ... 8.4328252e-01\n",
      "  8.4209108e-01 6.8990308e-01]\n",
      " [5.5457797e-04 2.3255087e-03 1.6916335e-03 ... 8.0783033e-01\n",
      "  8.2563221e-01 6.6535902e-01]\n",
      " [6.0504820e-04 2.1001147e-03 1.9804998e-03 ... 9.0649134e-01\n",
      "  8.6976004e-01 7.3983485e-01]], shape=(6, 2048), dtype=float32)\n",
      "tf.Tensor(\n",
      "[b'no acute disease' b'no acute cardiopulmonary abnormality'\n",
      " b'negative chest x xxxx' b'no acute cardiopulmonary abnormality'\n",
      " b'no acute cardiopulmonary abnormality' b'no acute pulmonary disease'], shape=(6,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for a, b in train_dataset:\n",
    "    print(a)\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = Input(shape=(2048), name='Image_1')\n",
    "dense1 = Dense(256, kernel_initializer=tf.keras.initializers.glorot_uniform(seed = 56),\n",
    "               name='dense_encoder')(input1)\n",
    "\n",
    "input2 = Input(shape=(155), name='Text_Input')\n",
    "emb_layer = Embedding(input_dim = vocab_size, output_dim = 300, input_length=155, mask_zero=True,\n",
    "                      trainable=False, name=\"Embedding_layer\") ##weights=[embedding_matrix], \n",
    "emb = emb_layer(input2)\n",
    "LSTM2 = LSTM(units=256, activation='tanh', recurrent_activation='sigmoid', use_bias=True, \n",
    "            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=23),\n",
    "            recurrent_initializer=tf.keras.initializers.orthogonal(seed=7),\n",
    "            bias_initializer=tf.keras.initializers.zeros(), name=\"LSTM2\")\n",
    "LSTM2_output = LSTM2(emb)\n",
    "dropout1 = Dropout(0.5, name='dropout1')(LSTM2_output)\n",
    "\n",
    "dec =  tf.keras.layers.Add()([dense1, dropout1])\n",
    "\n",
    "fc1 = Dense(256, activation='relu', kernel_initializer=tf.keras.initializers.he_normal(seed = 63),\n",
    "            name='fc1')\n",
    "fc1_output = fc1(dec)\n",
    "output_layer = Dense(vocab_size, activation='softmax', name='Output_layer')\n",
    "output = output_layer(fc1_output)\n",
    "\n",
    "encoder_decoder = Model(inputs = [input1, input2], outputs = output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = tf.keras.losses.CategoricalCrossentropy(from_logits=False, reduction='auto')\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    #getting mask value to not consider those words which are not present in the true caption\n",
    "    print(y_true)\n",
    "    mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
    "\n",
    "    loss_ = loss_function(y_true, y_pred)\n",
    "    \n",
    "    #converting mask dtype to loss_ dtype\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    \n",
    "    #applying the mask to loss\n",
    "    loss_ = loss_*mask\n",
    "    \n",
    "    #returning mean over all the values\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "encoder_decoder.compile(optimizer,loss = custom_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'Tensorboard/logs_m1/fit3/' + current_time + '/train'\n",
    "val_log_dir = 'Tensorboard/logs_m1/fit3/' + current_time + '/test'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "val_summary_writer = tf.summary.create_file_writer(val_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_56\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Text_Input (InputLayer)        [(None, 155)]        0           []                               \n",
      "                                                                                                  \n",
      " Embedding_layer (Embedding)    (None, 155, 300)     3000        ['Text_Input[0][0]']             \n",
      "                                                                                                  \n",
      " Image_1 (InputLayer)           [(None, 2048)]       0           []                               \n",
      "                                                                                                  \n",
      " LSTM2 (LSTM)                   (None, 256)          570368      ['Embedding_layer[0][0]']        \n",
      "                                                                                                  \n",
      " dense_encoder (Dense)          (None, 256)          524544      ['Image_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout1 (Dropout)             (None, 256)          0           ['LSTM2[0][0]']                  \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 256)          0           ['dense_encoder[0][0]',          \n",
      "                                                                  'dropout1[0][0]']               \n",
      "                                                                                                  \n",
      " fc1 (Dense)                    (None, 256)          65792       ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " Output_layer (Dense)           (None, 10)           2570        ['fc1[0][0]']                    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,166,274\n",
      "Trainable params: 1,163,274\n",
      "Non-trainable params: 3,000\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(encoder_decoder.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bytes_to_string(arr):\n",
    "    '''The generator gives provides data in bytes. This function converts them back to strings for manipulation'''\n",
    "    for i in range(len(arr)):\n",
    "        arr[i] = arr[i].decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2145: array([[4.9550453e-04, 2.0991680e-03, 1.7735970e-03, ..., 8.4199679e-01,\n",
       "         8.4065425e-01, 6.8691975e-01]], dtype=float32),\n",
       " 1051: array([[0.        , 0.00184126, 0.00116009, ..., 0.81290716, 0.81862867,\n",
       "         0.66535187]], dtype=float32),\n",
       " 1131: array([[4.6907488e-04, 2.1022649e-03, 2.4972956e-03, ..., 8.2906669e-01,\n",
       "         8.4035921e-01, 6.7798662e-01]], dtype=float32),\n",
       " 3616: array([[1.4932631e-04, 1.7147649e-03, 2.8314192e-03, ..., 8.4204483e-01,\n",
       "         8.4104896e-01, 6.8688482e-01]], dtype=float32),\n",
       " 3339: array([[7.0486767e-06, 2.1787314e-03, 1.8758981e-03, ..., 8.3596873e-01,\n",
       "         8.4641933e-01, 6.8505216e-01]], dtype=float32)}"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bytes_to_string(arr):\n",
    "    '''The generator gives provides data in bytes. This function converts them back to strings for manipulation'''\n",
    "    for i in range(len(arr)):\n",
    "        arr[i] = arr[i].decode('utf-8')\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(images, reports):\n",
    "    '''This function takes the batch of data and converts them into a new dataset(A WORD BY WORD DATASET)'''\n",
    "    imgs = []\n",
    "    in_reports = []\n",
    "    out_reports = []\n",
    "    sequence = []\n",
    "    for i in range(len(images)):\n",
    "        sequence = [tokenizer.word_index[e] for e in reports[i].split() if e in tokenizer.word_index.keys()]\n",
    "        for j in range(1,len(sequence)):\n",
    "\n",
    "            in_seq = sequence[:j]\n",
    "            out_seq = sequence[j]\n",
    "            out_seq = tf.keras.utils.to_categorical(out_seq, num_classes=vocab_size)\n",
    "\n",
    "            imgs.append(images[i])\n",
    "          #  print(in_seq)\n",
    "            in_reports.append(in_seq)\n",
    "           # print(out_seq)\n",
    "            out_reports.append(out_seq)\n",
    "        \n",
    "    return np.array(imgs), np.array(in_reports), np.array(out_reports)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_dataset = train_dataset\n",
    "##if have cv_dataset then comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH :  1\n",
      "20\n",
      "Training Loss: 0.3524944335222244,  Val Loss: 0.005373527606328328\n",
      "Time Taken for this Epoch : 0.6853089332580566 sec\n",
      "EPOCH :  2\n",
      "20\n",
      "Training Loss: 0.3472699075937271,  Val Loss: 0.006023310517009936\n",
      "Time Taken for this Epoch : 0.5130889415740967 sec\n",
      "EPOCH :  3\n",
      "20\n",
      "Training Loss: 0.3615592122077942,  Val Loss: 0.005815087964660243\n",
      "Time Taken for this Epoch : 0.4712870121002197 sec\n",
      "EPOCH :  4\n",
      "20\n",
      "Training Loss: 0.29299361258745193,  Val Loss: 0.005489720587144818\n",
      "Time Taken for this Epoch : 0.5298151969909668 sec\n",
      "EPOCH :  5\n",
      "20\n",
      "Training Loss: 0.2851252108812332,  Val Loss: 0.004782014117951979\n",
      "Time Taken for this Epoch : 0.7960128784179688 sec\n",
      "EPOCH :  6\n",
      "20\n",
      "Training Loss: 0.2732304558157921,  Val Loss: 0.005046719783230832\n",
      "Time Taken for this Epoch : 0.5855510234832764 sec\n",
      "EPOCH :  7\n",
      "20\n",
      "Training Loss: 0.27072934806346893,  Val Loss: 0.004308307902854786\n",
      "Time Taken for this Epoch : 0.4744250774383545 sec\n",
      "EPOCH :  8\n",
      "20\n",
      "Training Loss: 0.27688272297382355,  Val Loss: 0.004441075026988983\n",
      "Time Taken for this Epoch : 0.482072114944458 sec\n",
      "EPOCH :  9\n",
      "20\n",
      "Training Loss: 0.2932315096259117,  Val Loss: 0.004915854648539894\n",
      "Time Taken for this Epoch : 0.48651885986328125 sec\n",
      "EPOCH :  10\n",
      "20\n",
      "Training Loss: 0.28032365441322327,  Val Loss: 0.00435169995353933\n",
      "Time Taken for this Epoch : 0.5797550678253174 sec\n"
     ]
    }
   ],
   "source": [
    "epoch_train_loss = []\n",
    "epoch_val_loss = []\n",
    "EPOCH = 10\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    print('EPOCH : ',epoch+1)\n",
    "    batch_loss_tr = 0\n",
    "    batch_loss_vl = 0\n",
    "    start = time.time()\n",
    "    for img, report in train_dataset:\n",
    "       \n",
    "        r1 = bytes_to_string(report.numpy())\n",
    "        img_input, rep_input, output_word = convert(img.numpy(), r1)\n",
    "        rep_input = pad_sequences(rep_input, maxlen=155, padding='post')\n",
    "        \n",
    "        results = encoder_decoder.train_on_batch([img_input, rep_input], output_word)\n",
    "        \n",
    "        batch_loss_tr += results\n",
    "    print(X_train_img.shape[0])\n",
    "    train_loss = batch_loss_tr/(X_train_img.shape[0]//BATCH_SIZE)\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', train_loss, step = epoch)\n",
    "    \n",
    "    for img, report in cv_dataset:\n",
    "        \n",
    "        r1 = bytes_to_string(report.numpy())\n",
    "        img_input, rep_input, output_word = convert(img.numpy(), r1)\n",
    "        rep_input = pad_sequences(rep_input, maxlen=155, padding='post')\n",
    "        results = encoder_decoder.test_on_batch([img_input, rep_input], output_word)\n",
    "        batch_loss_vl += results\n",
    "        \n",
    "    val_loss = batch_loss_vl/(X_cv_img.shape[0]//14)\n",
    "    with val_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', val_loss, step = epoch)\n",
    "\n",
    "    epoch_train_loss.append(train_loss)\n",
    "    epoch_val_loss.append(val_loss)\n",
    "    \n",
    "    print('Training Loss: {},  Val Loss: {}'.format(train_loss, val_loss))\n",
    "    print('Time Taken for this Epoch : {} sec'.format(time.time()-start))   \n",
    "    encoder_decoder.save_weights('weights/BM7_new_model1_epoch_'+ str(epoch+1) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
